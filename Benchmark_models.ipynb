{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97aebc4f",
   "metadata": {},
   "source": [
    "# *Benchmark Models Generation - to provide a baseline for the prediction competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ba263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import os\n",
    "from functools import partial\n",
    "import random\n",
    "\n",
    "\n",
    "# Views 3\n",
    "# These imports require a certificate, these won't work without the certificate\n",
    "import views_runs\n",
    "from viewser.operations import fetch\n",
    "from views_forecasts.extensions import *\n",
    "from viewser import Queryset, Column\n",
    "\n",
    "#All the functions are in the BenchmarkModels.py file, importing them to use them in this notebook\n",
    "from BenchmarkModels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters:\n",
    "\n",
    "dev_id = 'Fatalities002' # This is used when using ensemble prediction functions later in the notebook - pd.DataFrame.forecasts.read_store(cm_ensemble_name, run=dev_id)[stepcols]\n",
    "run_id = 'Fatalities002'\n",
    "EndOfHistory = 508\n",
    "get_future = False\n",
    "\n",
    "username = os.getlogin() # This is your PC's username to use Dropbox - install Dropbox App to avail this functionality\n",
    "\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for - Creates a list from 1 to 36 with 36 numbers\n",
    "\n",
    "fi_steps = [1,3,6,12,36] # this list is never used\n",
    "\n",
    "\n",
    "# Specifying partitions\n",
    "\n",
    "calib_partitioner_dict = {\"train\":(121,396),\"predict\":(397,456)} #Calibration partition - the numbers are month codes 121 - January 1990 \n",
    "test_partitioner_dict = {\"train\":(121,444),\"predict\":(457,504)} #Test partition\n",
    "future_partitioner_dict = {\"train\":(121,492),\"predict\":(505,512)} #Future Prediction partition\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict}) # calling a function from viewser\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict}) # calling a function from viewser\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict}) # calling a function from viewser\n",
    "\n",
    "Mydropbox = f'/Users/{username}/Dropbox (ViEWS)/ViEWS/' #download Dropbox app\n",
    "overleafpath = f'/Users/{username}/Dropbox (ViEWS)/Apps/Overleaf/Prediction competition 2023/' #download Overleaf app\n",
    "\n",
    "\n",
    "print('Dropbox path set to',Mydropbox)\n",
    "print('Overleaf path set to',overleafpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639fc756",
   "metadata": {},
   "source": [
    "# From here we start creating benchmark models - we will create 1000 or 100 draws or probabilty distribution around the point predictions from the viewser prediction ensemble function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1983d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ln_ged_sb_dep', 'step_pred_3', 'step_pred_4', 'step_pred_5', 'step_pred_6', 'step_pred_7', 'step_pred_8', 'step_pred_9', 'step_pred_10', 'step_pred_11', 'step_pred_12', 'step_pred_13', 'step_pred_14']\n"
     ]
    }
   ],
   "source": [
    "# Benchmark model parameters \n",
    "filepath = Mydropbox + 'Prediction_competition_2023/'\n",
    "\n",
    "year_list = [2018, 2019, 2020, 2021] # Creating predictions for 4 years\n",
    "draws_cm = 1000 # 1000 draws at cm level\n",
    "draws_pgm = 100 # 100 draws at pgm level\n",
    "\n",
    "steps = [3,4,5,6,7,8,9,10,11,12,13,14] #list of steps to create a list of names below\n",
    "stepcols = ['ln_ged_sb_dep']\n",
    "for step in steps:\n",
    "    stepcols.append('step_pred_' + str(step))\n",
    "print(stepcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a9c1ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a787408",
   "metadata": {},
   "source": [
    "# CM Level Benchmark Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753bec8",
   "metadata": {},
   "source": [
    "### Based on ensemble from viewser prediction function; the point prediciton are expanded using a Poisson draw with mean=variance=\\hat{y}_{it} Mean and variance are equal. They are equal to the point prediction here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d09e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling benchmark based on VIEWS ensemble predictions\n",
    "sc_predictions_ensemble = [] #empty list to store the predictions\n",
    "cm_ensemble_name = 'cm_ensemble_genetic_test' #internal name to feed to the viewser function\n",
    "    \n",
    "ensemble_df = pd.DataFrame.forecasts.read_store(cm_ensemble_name, run=dev_id)[stepcols] #need viewser certificate, read_store is a function \n",
    "ensemble_df.head() # check the ensemble dataframe\n",
    "\n",
    "for year in year_list: # year_list = [2018, 2019, 2020, 2021]\n",
    "    sc_dict = {\n",
    "        'year': year,\n",
    "        'prediction_df': extract_sc_predictions(year=year,ss_predictions=ensemble_df)\n",
    "    } # creating a dictionary year-wise\n",
    "    sc_predictions_ensemble.append(sc_dict) # appended the dictionary to the empty list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff35e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting actuals from the ensemble\n",
    "actuals=np.expm1(ensemble_df['ln_ged_sb_dep'].fillna(0)) #using numpy Calculate exp(x) - 1 for all elements in the array on one column of pandas\n",
    "actuals_by_year=[] #year list\n",
    "for year in year_list:\n",
    "    actuals_dict = {\n",
    "        'year': year,\n",
    "        'actuals_df': extract_year(year=year,df=actuals) #extract info from actuals year-wise, see BenchmarkModels.py for the function details\n",
    "    }\n",
    "    actuals_by_year.append(actuals_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6078f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals.isna().sum() #finds the number of missing values in the dataframe - actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3c16d",
   "metadata": {},
   "source": [
    "### CM - Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d17741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding by drawing n draws from \"Poisson distribution\"   \n",
    "\n",
    "for year_record in sc_predictions_ensemble:\n",
    "    print(year_record['year'])\n",
    "    df = year_record.get('prediction_df')\n",
    "    year_record['expanded_df_poisson'] = expanded_df_poisson(df,ndraws=1000,level='cm') #create a new column in the \n",
    "    \n",
    "describe_expanded(df=sc_predictions_ensemble[0]['prediction_df'], df_expanded=sc_predictions_ensemble[0]['expanded_df_poisson'], month=457, country=57)  #comparing actuals with expanded dataframe   #month 457 - Jan 2018 and country 57? why this particularly? \n",
    "\n",
    "sc_predictions_ensemble[0]['expanded_df_poisson'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64296ad",
   "metadata": {},
   "source": [
    "### CM - Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280eba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding by drawing n draws bootstrap-fashion from the actuals   \n",
    "\n",
    "for year_record in actuals_by_year:\n",
    "#    print(year_record)\n",
    "    df = year_record.get('actuals_df')\n",
    "    \n",
    "    year_record['expanded_df_bootstrap'] = expanded_df_bootstrap(df,ndraws=1000,draw_from=df['ln_ged_sb_dep'],level='cm')\n",
    "    \n",
    "describe_expanded(df=actuals_by_year[0]['actuals_df'], df_expanded=actuals_by_year[0]['expanded_df_bootstrap'], month=457, country=57) #comparing actuals with expanded dataframe   #month 457 - Jan 2018 and country 57? why this particularly?\n",
    "\n",
    "actuals_by_year[0]['expanded_df_bootstrap'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcb7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals_by_year[0]['actuals_df']['ln_ged_sb_dep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b77fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped = pd.DataFrame(actuals_by_year[0]['expanded_df_bootstrap']['outcome']).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b9747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals_by_year[0]['actuals_df']['ln_ged_sb_dep'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9366d",
   "metadata": {},
   "source": [
    "### CM - Constituent Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bd551",
   "metadata": {},
   "source": [
    "#### Based on constituent models\n",
    "\n",
    "Short version, 20 models: \n",
    "1 \"draw\"\n",
    "from each of 20 constituent models\n",
    "\n",
    "Plus version with 45 draws from Poisson distribution for each model.\n",
    "\n",
    "Possibly obsolete:\n",
    "Long version, 440 models:\n",
    "20 \"draws\" from each of 22 constituent models, using predictions for adjacent steps (from s-4 to s+6). Some duplications to weight the most proximate steps more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fatalities002 stuff - contains the list of the current fatalities002 ensemble models\n",
    "\n",
    "from ModelDefinitions import DefineEnsembleModels\n",
    "\n",
    "level = 'cm'\n",
    "ModelList_cm = DefineEnsembleModels(level)\n",
    "ModelList_cm = ModelList_cm[0:20] # Drop Markov models\n",
    "\n",
    "i = 0\n",
    "for model in ModelList_cm:\n",
    "    print(i, model['modelname'], model['data_train'])\n",
    "    i = i + 1\n",
    "\n",
    "# Retrieving the predictions for calibration and test partitions\n",
    "# The ModelList contains the predictions organized by model\n",
    "from Ensembling import CalibratePredictions, RetrieveStoredPredictions, mean_sd_calibrated, gam_calibrated\n",
    "\n",
    "ModelList_cm = RetrieveStoredPredictions(ModelList_cm, steps, EndOfHistory, dev_id, level, get_future)\n",
    "\n",
    "ModelList_cm = CalibratePredictions(ModelList_cm, EndOfHistory, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling benchmark based on VIEWS constituent model predictions\n",
    "draws_per_model = np.floor_divide(draws_cm,len(ModelList_cm))\n",
    "\n",
    "for model in ModelList_cm:\n",
    "    print(model['modelname'])\n",
    "\n",
    "    model['sc_predictions_constituent'] = []\n",
    "\n",
    "    for year in year_list:\n",
    "        sc_dict = {\n",
    "            'year': year,\n",
    "            'prediction_df': extract_sc_predictions(year=year,ss_predictions=model['predictions_test_df'])\n",
    "        }\n",
    "        model['sc_predictions_constituent'].append(sc_dict)\n",
    "\n",
    "    # Expanding by drawing n draws from Poisson distribution   \n",
    "    for year_record in model['sc_predictions_constituent']:\n",
    "        print(year_record['year'])\n",
    "        df = year_record.get('prediction_df')\n",
    "        year_record['expanded_df'] = expanded_df(df,ndraws=50,level='cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4793e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_predictions_constituent = []\n",
    "\n",
    "for year in year_list:\n",
    "    print(year)\n",
    "    print(ModelList_cm[0]['modelname'])\n",
    "    merged_expanded_df = ModelList_cm[0]['sc_predictions_constituent'][year-2018]['expanded_df']\n",
    "#    print(expanded_df.describe())\n",
    "    i = 0\n",
    "    for model in ModelList_cm[1:19]:\n",
    "        print(model['modelname'])\n",
    "        merged_expanded_df = pd.concat([merged_expanded_df,model['sc_predictions_constituent'][year-2018]['expanded_df']])\n",
    "#        print(expanded_df.describe())\n",
    "        \n",
    "    sc_dict = {\n",
    "        'year': year,\n",
    "        'expanded_df': merged_expanded_df\n",
    "    }\n",
    "    sc_predictions_constituent.append(sc_dict)\n",
    "    i = i + 1\n",
    "       \n",
    "#sc_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247cf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21d26142",
   "metadata": {},
   "source": [
    "### Saving the CM benchmark models & actuals in separate Parquet files in Dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3996860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['ensemble','constituent']\n",
    "i = 0\n",
    "for bm_model in [sc_predictions_ensemble,sc_predictions_constituent]:\n",
    "    for record in bm_model:\n",
    "        year_record = record # First part of record list is list of yearly predictions, second is string name for benchmark model\n",
    "        print(year_record['year'])\n",
    "        filename = filepath + 'bm_cm_' + model_names[i] + '_expanded_' + str(year_record['year']) + '.parquet'\n",
    "        print(filename)\n",
    "        year_record['expanded_df'].to_parquet(filename) #save to file\n",
    "    i = i + 1\n",
    "\n",
    "# Dataframe with actuals\n",
    "df_actuals = pd.DataFrame(ModelList_cm[0]['predictions_test_df']['ln_ged_sb_dep'])\n",
    "cm_actuals = df_actuals\n",
    "cm_actuals['ged_sb'] = np.expm1(cm_actuals['ln_ged_sb_dep'])\n",
    "cm_actuals.drop(columns=['ln_ged_sb_dep'], inplace=True)\n",
    "print(cm_actuals.head())\n",
    "print(cm_actuals.tail())\n",
    "print(cm_actuals.describe())\n",
    "\n",
    "\n",
    "# Annual dataframes with actuals, saved to disk\n",
    "for year in year_list:\n",
    "    first_month = (year - 1980)*12 + 1\n",
    "    last_month = (year - 1980 + 1)*12\n",
    "    df_annual = cm_actuals.loc[first_month:last_month]\n",
    "    filename = filepath + 'cm_actuals_' + str(year) + '.parquet'\n",
    "    print(year, first_month, last_month, filename)\n",
    "    print(df_annual.head())\n",
    "    df_annual.to_parquet(filename)\n",
    "# For all four years\n",
    "filename = filepath + 'cm_actuals_allyears.parquet' #save to file\n",
    "cm_actuals.to_parquet(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f670157",
   "metadata": {},
   "source": [
    "# PGM Level Benchmark Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77004edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling benchmark based on VIEWS ensemble predictions\n",
    "sc_predictions_ensemble_pgm = []\n",
    "# any old pgm data\n",
    "pgm_ensemble_name = 'pgm_ensemble_cm_calib_test'\n",
    "    \n",
    "ensemble_pgm_df = pd.DataFrame.forecasts.read_store(pgm_ensemble_name, run=dev_id)[stepcols]\n",
    "ensemble_pgm_df.head()\n",
    "\n",
    "for year in year_list[0:3]:\n",
    "    sc_dict = {\n",
    "        'year': year,\n",
    "        'prediction_df': extract_sc_predictions(year=year,ss_predictions=ensemble_pgm_df)\n",
    "    }\n",
    "    sc_predictions_ensemble_pgm.append(sc_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae8c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding by drawing n draws from Poisson distribution   \n",
    "function_with_draws = partial(sample_poisson_row, ndraws=500) #here 500 draws are used?\n",
    "for year_record in sc_predictions_ensemble_pgm:\n",
    "    print(year_record['year'])\n",
    "    df = year_record.get('prediction_df')\n",
    "    year_record['expanded_df'] = expanded_df(df,ndraws=500,level='pgm')\n",
    "\n",
    "#describe_expanded(df=sc_predictions_ensemble_pgm[0]['prediction_df'], df_expanded=sc_predictions_ensemble_pgm[0]['expanded_df'], month=457, country=57)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ba9569",
   "metadata": {},
   "source": [
    "# Saving the pgm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['ensemble','constituent']\n",
    "i = 0\n",
    "for bm_model in [sc_predictions_ensemble_pgm]:\n",
    "    for record in bm_model:\n",
    "        year_record = record # First part of record list is list of yearly predictions, second is string name for benchmark model\n",
    "        print(year_record['year'])\n",
    "        filename = filepath + 'bm_pgm_' + model_names[i] + '_expanded_' + str(year_record['year']) + '.parquet'\n",
    "        print(filename)\n",
    "        year_record['expanded_df'].to_parquet(filename)\n",
    "    i = i + 1\n",
    "\n",
    "# Dataframe with actuals\n",
    "df_actuals = pd.DataFrame(ensemble_pgm_df)\n",
    "pgm_actuals = df_actuals\n",
    "pgm_actuals['ged_sb'] = np.expm1(pgm_actuals['ln_ged_sb_dep'])\n",
    "pgm_actuals.drop(columns=['ln_ged_sb_dep'], inplace=True)\n",
    "print(pgm_actuals.head())\n",
    "print(pgm_actuals.tail())\n",
    "print(pgm_actuals.describe())\n",
    "\n",
    "\n",
    "# Annual dataframes with actuals, saved to disk\n",
    "for year in year_list:\n",
    "    first_month = (year - 1980)*12 + 1\n",
    "    last_month = (year - 1980 + 1)*12\n",
    "    df_annual = pgm_actuals.loc[first_month:last_month]\n",
    "    filename = filepath + 'cm_actuals_' + str(year) + '.parquet'\n",
    "    print(year, first_month, last_month, filename)\n",
    "    print(df_annual.head())\n",
    "    df_annual.to_parquet(filename)\n",
    "# For all four years\n",
    "filename = filepath + 'pgm_actuals_allyears.parquet'\n",
    "pgm_actuals.to_parquet(filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d686ea",
   "metadata": {},
   "source": [
    "# Old stuff from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_data = sc_predictions_ensemble[0].get('prediction_df')\n",
    "test_data['draws'] = test_data.apply(function_with_draws, axis=1)\n",
    "test_data.explode('draws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fef3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test version, cm\n",
    "test_data = sc_predictions_ensemble[0].get('prediction_df')\n",
    "test_data['draws'] = test_data.apply(function_with_draws, axis=1)\n",
    "td = test_data.explode('draws')\n",
    "td.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6bcac5",
   "metadata": {},
   "source": [
    "# Old cm stuff from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe118d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_df_cm(df, draw):\n",
    "    ''' Drops steps we will not need in the benchmark model. \n",
    "    Another round of drops are done below '''\n",
    "    steps_to_drop = ['ln_ged_sb_dep','step_pred_25','step_pred_26','step_pred_27','step_pred_28','step_pred_29','step_pred_30',\n",
    "                     'step_pred_31','step_pred_32','step_pred_33','step_pred_34','step_pred_35','step_pred_36',]\n",
    "    df = df.drop(steps_to_drop,axis=1)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['draw'] = draw\n",
    "    df_long = pd.wide_to_long(df, 'step_pred_', i = ['month_id', 'country_id'], j = 'step')\n",
    "    df_long.reset_index(inplace=True)\n",
    "    df_long.set_index(['month_id','country_id','step','draw'],inplace=True)\n",
    "    return(df_long)\n",
    "    \n",
    "model_draw = 0\n",
    "df = ModelList_cm[model_draw]['predictions_test_df'].copy()\n",
    "df_cm_results_long = reshape_df_cm(df,model_draw)\n",
    "print('Starting with model/draw',model_draw, model['modelname'])\n",
    "print(df_cm_results_long.describe())\n",
    "print(df_cm_results_long.head())\n",
    "\n",
    "\n",
    "for model in ModelList_cm[1:]:\n",
    "    model_draw += 1\n",
    "    print('Appending model/draw',model_draw, model['modelname'])\n",
    "    df = ModelList_cm[model_draw]['predictions_test_df'].copy()\n",
    "    df_reshaped = reshape_df_cm(df,model_draw)\n",
    "    df_cm_results_long = pd.concat([df_cm_results_long ,df_reshaped], axis=0)\n",
    "    \n",
    "\n",
    "df_cm_results_long['prediction'] = np.round_(np.expm1(df_cm_results_long['step_pred_'])).astype('int32')\n",
    "df_cm_results_long.drop(columns=['step_pred_'], inplace=True)\n",
    "# Results file in long format\n",
    "print(df_cm_results_long.describe())\n",
    "print(df_cm_results_long.tail())\n",
    "\n",
    "print(df_cm_results_long.loc[492].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1871a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm_final_extended = df_cm_results_long.copy()\n",
    "\n",
    "def make_dfcopy_cm(df_in, step, shifted_step, repetition):\n",
    "    ''' Makes a 'copy' of the df with a shifted step '''\n",
    "#    print(step, shifted_step, repetition)\n",
    "    df = pd.DataFrame(df_in[df_in.index.get_level_values('step').isin([shifted_step])]).copy()\n",
    "    df.reset_index(inplace = True)\n",
    "    df['step'].replace(shifted_step, step, inplace = True)\n",
    "#    print(df.describe())\n",
    "    df['draw'] = (df['draw'] + len(ModelList_cm) * repetition)\n",
    "#    print(df.describe())\n",
    "    df.set_index(['month_id', 'country_id', 'step', 'draw'], inplace=True)\n",
    "    return(df)\n",
    "    \n",
    "df_list_steps = []\n",
    "df_list_steps.append(df_cm_results_long)\n",
    "for step in range(3,14+1):     \n",
    "#    print(80*'*')\n",
    "    print('Step', step)\n",
    "    df = pd.DataFrame(df_cm_results_long[df_cm_results_long.index.get_level_values('step').isin([step])])\n",
    "#    print(df.head(3))\n",
    "    repetition = 1\n",
    "    df_list = []\n",
    "    for shift in [(-4,2),(-3,4),(-2,5),(-1,6),(0,6),(1,5),(2,4),(3,3),(4,2),(5,2),(6,2),(7,1),(8,1),(9,1)]:\n",
    "        for copy in range(1,shift[1]+1):\n",
    "            shifted_step = step+shift[0]\n",
    "            if shifted_step < 1:\n",
    "                shifted_step = step\n",
    "            step_list = [shifted_step]\n",
    "            df = make_dfcopy_cm(df_in = df_cm_results_long,step = step, shifted_step = shifted_step, repetition = repetition)\n",
    "            df_list.append(df)\n",
    "            repetition += 1\n",
    "    df_cm_temp = pd.concat(df_list)\n",
    "    df_list_steps.append(df_cm_temp)\n",
    "\n",
    "df_cm_final_extended = pd.concat(df_list_steps)\n",
    "#df.reorder_levels(['month_id','country_id','steps','draw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb476c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_ss48_to_sc12(df, level,firstmonth,years):\n",
    "    ''' Converts a dataframe in long format from one including all VIEWS ss predictions \n",
    "        into a set of dataframes containing only sc predictions for 12 months '''\n",
    "    df_list = []\n",
    "    for year in range(1,years+1):\n",
    "        this_firstmonth = firstmonth + (year-1)*12\n",
    "        print(year, this_firstmonth)\n",
    "#        this_df = df.query(f'month_id >= {this_firstmonth} and month_id <= {this_firstmonth+12-1}')\n",
    "        month_df_list = []\n",
    "        for step in range(3,14+1):\n",
    "            select_month = this_firstmonth + step - 3\n",
    "#            print('retaining month',select_month,'step',step)\n",
    "            month_df = df.query(f'month_id == {select_month} and step == {step}')\n",
    "            month_df_list.append(month_df)\n",
    "        year_df = pd.concat(month_df_list)\n",
    "        df_list.append(year_df)\n",
    "    return(df_list)\n",
    "\n",
    "    \n",
    "cm_ensemble_predictions = from_ss48_to_sc12(df_cm_final_extended,'cm',445,4)\n",
    "\n",
    "cm_ensemble_predictions[1].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d92db",
   "metadata": {},
   "source": [
    "### cm last historical values benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c89361",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = (Queryset(\"benchmark_cm\", \"country_month\")\n",
    "\n",
    "   # target variable\n",
    "   .with_column(Column(\"ged_sb\", from_table=\"ged2_cm\", from_column=\"ged_sb_best_sum_nokgi\")\n",
    "                .transform.missing.fill()\n",
    "                .transform.missing.replace_na()\n",
    "                )\n",
    "\n",
    "\n",
    "   .with_theme(\"benchmark\")\n",
    "   .describe(\"\"\"Data for empirical benchmark model, cm level\n",
    "\n",
    "\n",
    "            \"\"\")\n",
    "   )\n",
    "\n",
    "#queryset = Queryset(\"name\", \"loa\") # if not already defined\n",
    "column = \"ged_sb_best_sum_nokgi\"\n",
    "table = \"ged2_cm\"\n",
    "lags=range(1,65)\n",
    "for lag in lags: \n",
    "    qs = qs.with_column(Column(column+'_' + str(lag), from_table = table, from_column=column)\n",
    "                        .transform.missing.replace_na()\n",
    "                        .transform.temporal.tlag(lag)\n",
    "                       )\n",
    "df_cm_historical_values = qs.publish().fetch()\n",
    "df_cm_historical_values = df_cm_historical_values.loc[445:492]\n",
    "\n",
    "df_cm_historical_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating predictions for test partition\n",
    "number_of_lags = 45\n",
    "maxstep = 14\n",
    "df_list_bystep = []\n",
    "for step in range(3,maxstep+1):\n",
    "    lags=range(step,step + number_of_lags)\n",
    "    draw = 0\n",
    "    print('step:',step,'lag:',lag, 'draw:',draw)\n",
    "    df_list = []\n",
    "    for lag in lags:\n",
    "        number_of_repetitions = number_of_lags+step-lag\n",
    "#        print('lag:',lag,'repetitions:',number_of_repetitions)\n",
    "#        print('step:',step,'lag:',lag, 'draw:',draw)\n",
    "        for repetition in range(1,number_of_repetitions):\n",
    "            lagged_col = 'ged_sb_best_sum_nokgi_' + str(lag)\n",
    "            df = pd.DataFrame(df_cm_historical_values[lagged_col].copy())\n",
    "            df.reset_index(inplace=True)\n",
    "            df['prediction'] = df[lagged_col]\n",
    "#            print(df.head())\n",
    "            df.drop(columns=[lagged_col], inplace=True)\n",
    "            df['step'] = step\n",
    "            df['draw'] = draw\n",
    "            df.set_index(['month_id', 'country_id', 'step','draw'], inplace=True)\n",
    "            df_list.append(df)\n",
    "#            if draw == 1 and step == 1:\n",
    "#                df_cm_predictions_historical_values = df.copy()\n",
    "#            else:\n",
    "#                df_cm_predictions_historical_values = pd.concat([df_cm_predictions_historical_values,df])\n",
    "            draw = draw + 1\n",
    "    df_cm_predictions_lag = pd.concat(df_list)\n",
    "    df_list_bystep.append(df_cm_predictions_lag)\n",
    "    print('Number of draws:',draw + 1)\n",
    "df_cm_predictions_historical_values = pd.concat(df_list_bystep) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cm_predictions_historical_values.describe())\n",
    "print(df_cm_predictions_historical_values.head())\n",
    "print(df_cm_predictions_historical_values.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_historical_values_predictions = from_ss48_to_sc12(df_cm_predictions_historical_values,'cm',445,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_categorize(df, level):\n",
    "    ''' This function aggregates the input df across all draws, and returns summary statistics for the prediction model '''\n",
    "    if level == 'cm':\n",
    "        index = ['month_id','country_id']\n",
    "    if level == 'pgm':\n",
    "        index = ['month_id', 'priogrid_gid']\n",
    "    if level == 'pgm2':\n",
    "        index = ['month_id', 'priogrid_id']\n",
    "    df_to_aggregate = df.copy()\n",
    "    df_to_aggregate['log_prediction'] = np.log1p(df_to_aggregate['prediction'] )\n",
    "\n",
    "    # Proportion of draws in fatality categories\n",
    "    #for cutoffs in [0,1,10,100,1000,10000]:\n",
    "    bins = pd.IntervalIndex.from_tuples([(-1, 0), (1, 10), (11, 100), (101, 1000), (1001, 10000), (10001,100000000)])\n",
    "    df_to_aggregate['categorical'] = pd.cut(df_to_aggregate['prediction'],bins)\n",
    "    df_to_aggregate_dummies = pd.get_dummies(df_to_aggregate['categorical'],prefix='cat')\n",
    "    df_to_aggregate = pd.concat([df_to_aggregate,df_to_aggregate_dummies],axis=1)\n",
    "\n",
    "    # Mean and standard deviation of log predictions\n",
    "    df_aggregated = pd.DataFrame(df_to_aggregate['log_prediction'].groupby(level=index).mean())\n",
    "    df_aggregated.rename(columns={'log_prediction':'mean_log_prediction'},inplace=True)\n",
    "    df_aggregated['std_log_prediction'] = df_to_aggregate['log_prediction'].groupby(level=index).std()\n",
    "    for col in ('cat_(-1, 0]','cat_(1, 10]','cat_(11, 100]','cat_(101, 1000]','cat_(1001, 10000]','cat_(10001, 100000000]'):\n",
    "        df_aggregated[col] = df_to_aggregate[col].groupby(level=index).mean()\n",
    "    return(df_aggregated)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate across draws/samples to extract means, standard deviations, and category probabilities\n",
    "df_cm_predictions_historical_values_aggregated = aggregate_and_categorize(df_cm_predictions_historical_values,'cm')\n",
    "df_cm_predictions_ensemble_aggregated = aggregate_and_categorize(df_cm_final_extended,'cm')\n",
    "\n",
    "print(df_cm_predictions_historical_values_aggregated.describe())\n",
    "print(df_cm_predictions_historical_values_aggregated.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Saving the annual sc files and the aggregated versions of them\n",
    "filepath = Mydropbox + 'Prediction_competition_2023/'\n",
    "year = 2018\n",
    "for df in cm_historical_values_predictions:\n",
    "    # Simplifying the indices: removing the step column\n",
    "    print('cm_historical', year)\n",
    "    df = df.reset_index()\n",
    "    df.set_index(['month_id','country_id','draw'],inplace=True)\n",
    "    df.drop(columns=['step'],inplace=True)\n",
    "    df['prediction'] = df['prediction'].astype('int32') \n",
    "    print(df.head())\n",
    "    print(df.dtypes)\n",
    "    filename = filepath + 'bm_cm_historical_values_' + str(year) + '.parquet'\n",
    "    print(filename)\n",
    "    df.to_parquet(filename)\n",
    "    df_aggregated = aggregate_and_categorize(df,'cm')\n",
    "    filename = filepath + 'bm_cm_historical_values_agg' + str(year) + '.parquet'\n",
    "    df_aggregated.to_parquet(filename)\n",
    "    year = year + 1\n",
    "year = 2018\n",
    "for df in cm_ensemble_predictions:\n",
    "    print('cm_ensemble', year)\n",
    "    df = df.reset_index()\n",
    "    df.set_index(['month_id','country_id','draw'],inplace=True)\n",
    "    df.drop(columns=['step'],inplace=True)\n",
    "    df['prediction'] = df['prediction'].astype('int32') \n",
    "    filename = filepath + 'bm_cm_ensemble_' + str(year) + '.parquet'\n",
    "    print(filename)\n",
    "    df.to_parquet(filename)\n",
    "    df_aggregated = aggregate_and_categorize(df,'cm')\n",
    "    filename = filepath + 'bm_cm_ensemble_agg' + str(year) + '.parquet'\n",
    "    df_aggregated.to_parquet(filename)\n",
    "    year = year + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13036c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947356fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "include_expansive = False\n",
    "\n",
    "if include_expansive:\n",
    "    filename = Mydropbox + 'Prediction_competition_2023/' + 'cm_actuals.parquet'\n",
    "    cm_actuals.to_parquet(filename)\n",
    "    filename = Mydropbox + 'Prediction_competition_2023/' + 'cm_benchmark_ensemble_22.parquet'\n",
    "    df_cm_results_long_pruned.to_parquet(filename)\n",
    "    filename = Mydropbox + 'Prediction_competition_2023/' + 'cm_benchmark_ensemble_550.parquet'\n",
    "    df_cm_final_extended.to_parquet(filename)\n",
    "    filename = Mydropbox + 'Prediction_competition_2023/' + 'cm_benchmark_ensemble_550_aggregated.parquet'\n",
    "    df_cm_predictions_ensemble_aggregated.to_parquet(filename)\n",
    "    filename = Mydropbox + 'Prediction_competition_2023/' + 'cm_benchmark_historical_values.parquet'\n",
    "    df_cm_predictions_historical_values.to_parquet(filename)\n",
    "    filename = Mydropbox + 'Prediction_competition_2023/' + 'cm_predictions_historical_values_aggregated.parquet'\n",
    "    df_cm_predictions_historical_values_aggregated.to_parquet(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51884343",
   "metadata": {},
   "source": [
    "# pgm level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4178e6ae",
   "metadata": {},
   "source": [
    "### Based on ensemble; expanded using a Poisson draw with mean=variance=\\hat{y}_{it}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling benchmark based on VIEWS ensemble predictions\n",
    "sc_predictions_ensemble_pgm = []\n",
    "pgm_ensemble_name = 'pgm_ensemble_cm_calib_test'\n",
    "    \n",
    "ensemble_pgm_df = pd.DataFrame.forecasts.read_store(pgm_ensemble_name, run=dev_id)[stepcols]\n",
    "ensemble_pgm_df.head()\n",
    "\n",
    "for year in year_list[0:3]:\n",
    "    sc_dict = {\n",
    "        'year': year,\n",
    "        'prediction_df': extract_sc_predictions(year=year,ss_predictions=ensemble_pgm_df)\n",
    "    }\n",
    "    sc_predictions_ensemble_pgm.append(sc_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5889e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pgm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb07bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding by drawing n draws from Poisson distribution   \n",
    "for year_record in sc_predictions_ensemble_pgm:\n",
    "    print(year_record['year'])\n",
    "    year_record['expanded_df'] = expanded_predictions(sc_predictions = year_record['prediction_df'],draws = draws_pgm, level = 'pgm')\n",
    "\n",
    "describe_expanded(df=sc_predictions_ensemble[0]['prediction_df'], df_expanded=sc_predictions_ensemble[0]['expanded_df'], month=457, country=57)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b14afc",
   "metadata": {},
   "source": [
    "# Old pgm stuff from here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea38184",
   "metadata": {},
   "source": [
    "## Ensemble model pgm benchmark\n",
    "\n",
    "kj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e78ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "level = 'pgm'\n",
    "ModelList_pgm = DefineEnsembleModels(level)\n",
    "    \n",
    "i = 0\n",
    "for model in ModelList_pgm:\n",
    "    print(i, model['modelname'], model['data_train'])\n",
    "    i = i + 1\n",
    "    \n",
    "# Retrieving the predictions for calibration and test partitions\n",
    "# The ModelList contains the predictions organized by model\n",
    "from Ensembling import CalibratePredictions, RetrieveStoredPredictions, mean_sd_calibrated, gam_calibrated\n",
    "\n",
    "ModelList_pgm = RetrieveStoredPredictions(ModelList_pgm, steps, EndOfHistory, dev_id, level, get_future)\n",
    "\n",
    "#ModelList_pgm = CalibratePredictions(ModelList_pgm, EndOfHistory, steps)\n",
    "\n",
    "# Dataframe with actuals\n",
    "df_actuals_pgm = pd.DataFrame(ModelList_pgm[0]['predictions_test_df']['ln_ged_sb_dep'])\n",
    "print(df_actuals_pgm.head())\n",
    "print(df_actuals_pgm.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48259249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "def reshape_df_pgm(df, draw):\n",
    "    ''' Drops steps we will not need in the benchmark model. \n",
    "    Another round of drops are done below '''\n",
    "    steps_to_drop = ['ln_ged_sb_dep','step_pred_23','step_pred_24',\n",
    "                     'step_pred_25','step_pred_26','step_pred_27','step_pred_28','step_pred_29','step_pred_30',\n",
    "                     'step_pred_31','step_pred_32','step_pred_33','step_pred_34','step_pred_35','step_pred_36',]\n",
    "    df = df.drop(steps_to_drop,axis=1)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['draw'] = draw\n",
    "    df_long = pd.wide_to_long(df, 'step_pred_', i = ['month_id', 'priogrid_id', 'draw'], j = 'step')\n",
    "    return(df_long)\n",
    "\n",
    "model_draw = 0\n",
    "df = ModelList_pgm[model_draw]['predictions_test_df'].copy()\n",
    "df_pgm_results_long = reshape_df_pgm(df,model_draw)\n",
    "print('Starting with model/draw',model_draw, model['modelname'])\n",
    "print(df_pgm_results_long.describe())\n",
    "\n",
    "\n",
    "for model in ModelList_pgm[1:]:\n",
    "    model_draw += 1\n",
    "    print('Appending model/draw',model_draw, model['modelname'])\n",
    "    df = ModelList_pgm[model_draw]['predictions_test_df'].copy()\n",
    "    df_reshaped = reshape_df_pgm(df,model_draw)\n",
    "    df_pgm_results_long = pd.concat([df_pgm_results_long ,df_reshaped], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pgm_results_long['prediction'] = np.round_(np.expm1(df_pgm_results_long['step_pred_'])).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fcee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pgm_results_long.drop(columns=['step_pred_'], inplace=True)\n",
    "#df_pgm_results_extended.index.set_names('priogrid_gid', level=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd78f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results file in long format\n",
    "print(df_pgm_results_long.describe())\n",
    "print(df_pgm_results_long.tail())\n",
    "\n",
    "print(df_pgm_results_long.loc[492].describe())\n",
    "\n",
    "# Extending by copying adjacent steps\n",
    "\n",
    "df_pgm_results_extended=df_pgm_results_long.copy()\n",
    "\n",
    "print(df_pgm_results_extended.describe())\n",
    "print(df_pgm_results_extended.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba44b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into separate files by step\n",
    "df_ensembles_pgm_by_step = []\n",
    "for step in range(3,14+1):\n",
    "    print(step)\n",
    "    df = df_pgm_results_extended.xs(step, level=3).copy()\n",
    "    #print(df.describe())\n",
    "    df_ensembles_pgm_by_step.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4549533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dfcopy_pgm(df_in, step, shifted_step, repetition):\n",
    "    ''' Makes a 'copy' of the df with a shifted step '''\n",
    "#    print(step, shifted_step, repetition)\n",
    "    df = pd.DataFrame(df_in[df_in.index.get_level_values('step').isin([shifted_step])]).copy()\n",
    "    df.reset_index(inplace = True)\n",
    "    df['step'].replace(shifted_step, step, inplace = True)\n",
    "#    print(df.describe())\n",
    "    df['draw'] = (df['draw'] + len(ModelList_pgm) * repetition)\n",
    "#    print(df.describe())\n",
    "    df.set_index(['month_id', 'priogrid_id', 'step', 'draw'], inplace=True)\n",
    "    return(df)\n",
    "\n",
    "for step in range(3,14+1):     \n",
    "    print(80*'*')\n",
    "    print('Step', step, '-- Original dataframe for step', step, 'is:')\n",
    "    df = pd.DataFrame(df_pgm_results_extended[df_pgm_results_extended.index.get_level_values('step').isin([step])])\n",
    "    print(df.describe())\n",
    "    repetition = 1\n",
    "    df_list_pgm = []\n",
    "    for shift in [(-4,1),(-3,1),(-2,3),(-1,4),(0,3),(1,3),(2,2),(3,2),(4,1),(5,1),(6,1),(7,1),(8,1)]:\n",
    "        for copy in range(1,shift[1]+1):\n",
    "            shifted_step = step+shift[0]\n",
    "            if shifted_step < 1:\n",
    "                shifted_step = step\n",
    "            step_list = [shifted_step]\n",
    "            df = make_dfcopy_pgm(df_in = df_pgm_results_extended,step = step, shifted_step = shifted_step, repetition = repetition)\n",
    "            df_list_pgm.append(df)\n",
    "            repetition += 1\n",
    "    df_pgm_temp = pd.concat(df_list_pgm)\n",
    "    print('Extended:')\n",
    "    print(df_pgm_temp.describe())\n",
    "    # Export to parquet\n",
    "    filename = Mydropbox + 'Prediction_competition_2023/' + 'pgm_benchmark_ensemble_step_' + str(step) + '.parquet'\n",
    "    df_pgm_temp.to_parquet(filename)\n",
    "    # Aggregate across draws, save    \n",
    "    filename = Mydropbox + 'Prediction_competition_2023/' + 'pgm_benchmark_ensemble_step_' + str(step) + '_aggregated.parquet'\n",
    "    df_aggregated_pgm = aggregate_and_categorize(df_pgm_temp,'pgm2')\n",
    "    print('Aggregated:')\n",
    "    print(df_aggregated_pgm.describe())\n",
    "    df_aggregated_pgm.to_parquet(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sc prediction files for ensemble model\n",
    "pgm_ensemble_predictions = from_ss48_to_sc12(df_pgm_results_extended,'pgm2',445,4)\n",
    "\n",
    "# Saving the annual sc files and the aggregated versions of them\n",
    "filepath = Mydropbox + 'Prediction_competition_2023/'\n",
    "year = 2018\n",
    "for df in pgm_ensemble_predictions:\n",
    "    filename = filepath + 'bm_pgm_ensemble_' + str(year) + '.parquet'\n",
    "    print(filename)\n",
    "    df.to_parquet(filename)\n",
    "    df_aggregated = aggregate_and_categorize(df,'pgm')\n",
    "    filename = filepath + 'bm_pgm_ensemble_agg' + str(year) + '.parquet'\n",
    "    df_aggregated.to_parquet(filename)\n",
    "    year = year + 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be4904",
   "metadata": {},
   "source": [
    "## Historical values pgm benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55417594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag settings (number of temporal lags at each spatial lag level)\n",
    "tlags_cell = 40\n",
    "tlags_firstorder = 27\n",
    "tlags_secondorder = 21\n",
    "\n",
    "# Spatial lags, first-order lag 1:\n",
    "kernel_inner=1\n",
    "kernel_width=1\n",
    "kernel_power=0\n",
    "norm_kernel=0\n",
    "\n",
    "rerun_querysets = False\n",
    "\n",
    "def retrieve_qs(qs_to_retrieve=qs,rerun=True,filename=''):\n",
    "    if rerun:\n",
    "        df = qs_to_retrieve.publish().fetch().loc[445:492]    \n",
    "        df.to_parquet(filename)\n",
    "    else:\n",
    "        df = pd.read_parquet(filename)\n",
    "    return(df)\n",
    "    \n",
    "\n",
    "print('Retrieving data for inner cells')\n",
    "\n",
    "column = \"ged_sb_best_sum_nokgi\"\n",
    "table = \"ged2_pgm\"\n",
    "\n",
    "qs = (Queryset(\"benchmark_pgm\", \"priogrid_month\")\n",
    "\n",
    "   # target variable at t0\n",
    "   .with_column(Column(\"ged_sb\", from_table=\"ged2_pgm\", from_column=\"ged_sb_best_sum_nokgi\")\n",
    "                .transform.missing.fill()\n",
    "                .transform.missing.replace_na()\n",
    "                )\n",
    "    # spatial lag at t0\n",
    "   .with_column(Column(\"splag_ged_sb_0\", from_table = table, from_column = column)\n",
    "                     .transform.missing.replace_na()\n",
    "                     .transform.spatial.lag(kernel_inner,kernel_width,kernel_power,norm_kernel)\n",
    "                    )\n",
    "\n",
    "\n",
    "   .with_theme(\"benchmark\")\n",
    "   .describe(\"\"\"Data for empirical benchmark model, pgm level\n",
    "            \"\"\")\n",
    "   )\n",
    "\n",
    "#queryset = Queryset(\"name\", \"loa\") # if not already defined\n",
    "column = \"ged_sb_best_sum_nokgi\"\n",
    "table = \"ged2_pgm\"\n",
    "tlags_0=range(1,tlags_cell + 1)\n",
    "qs0 = qs.copy()\n",
    "for lag in tlags_0: \n",
    "    qs0 = qs0.with_column(Column(column + '_' + str(lag), from_table = table, from_column=column)\n",
    "                        .transform.missing.replace_na()\n",
    "                        .transform.temporal.tlag(lag)\n",
    "                       )\n",
    "    \n",
    "filename = Mydropbox + 'Prediction_competition_2023/' + 'df_pgm_historical_values_0.parquet'\n",
    "df_pgm_historical_values_0 = retrieve_qs(qs_to_retrieve=qs0,rerun = rerun_querysets,filename=filename)\n",
    "\n",
    "#if rerun_querysets:\n",
    "#    df_pgm_historical_values_0 = qs0.publish().fetch().loc[445:492]    \n",
    "#    df_pgm_historical_values_0.to_parquet(filename)\n",
    "#else:\n",
    "#    df_pgm_historical_values_0 = pd.read_parquet(filename)\n",
    "\n",
    "# Spatial lags, first-order:\n",
    "print('Retrieving data for first-order neighbors')\n",
    "\n",
    "kernel_inner=1\n",
    "kernel_width=1\n",
    "kernel_power=0\n",
    "norm_kernel=0\n",
    "\n",
    "tlags_1=range(1,tlags_firstorder + 1)\n",
    "qs1 = qs.copy()\n",
    "for lag in tlags_1:\n",
    "    qs1 = qs1.with_column(Column(column + '_splag_1_' + str(lag), from_table = table, from_column=column)\n",
    "                        .transform.missing.replace_na()\n",
    "                        .transform.temporal.tlag(lag)\n",
    "                        .transform.spatial.lag(kernel_inner,kernel_width,kernel_power,norm_kernel)\n",
    "                       )\n",
    "\n",
    "filename = Mydropbox + 'Prediction_competition_2023/' + 'df_pgm_historical_values_1.parquet'\n",
    "#df_pgm_historical_values_1 = qs1.publish().fetch().loc[445:492]\n",
    "df_pgm_historical_values_1 = retrieve_qs(qs_to_retrieve=qs1,rerun = rerun_querysets,filename=filename)\n",
    "\n",
    "# Spatial lags; second-order:\n",
    "print('Retrieving data for second-order neighbors')\n",
    "\n",
    "kernel_inner=2\n",
    "kernel_width=1\n",
    "kernel_power=0\n",
    "norm_kernel=0\n",
    "\n",
    "tlags_2=range(1,tlags_secondorder + 1)\n",
    "qs2 = qs.copy()\n",
    "for lag in tlags_2:\n",
    "    qs2 = qs2.with_column(Column(column + '_splag_2_' + str(lag), from_table = table, from_column=column)\n",
    "                        .transform.missing.replace_na()\n",
    "                        .transform.temporal.tlag(lag)\n",
    "                        .transform.spatial.lag(kernel_inner,kernel_width,kernel_power,norm_kernel)\n",
    "                       )\n",
    "\n",
    "filename = Mydropbox + 'Prediction_competition_2023/' + 'df_pgm_historical_values_2.parquet'\n",
    "df_pgm_historical_values_2 = retrieve_qs(qs_to_retrieve=qs2,rerun = rerun_querysets,filename=filename)\n",
    "\n",
    "#df_pgm_historical_values_2 = qs2.publish().fetch().loc[445:492]\n",
    "print('Done retrieving data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aabc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pgm_historical_values_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ccd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data frames\n",
    "df_pgm_historical_values = pd.concat([df_pgm_historical_values_0, df_pgm_historical_values_1, df_pgm_historical_values_2], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pgm_historical_values = df_pgm_historical_values.loc[445:492]\n",
    "# Computing averages from sums\n",
    "for lag in tlags_1:\n",
    "    col = column + '_splag_1_' + str(lag)\n",
    "    df_pgm_historical_values[col] = df_pgm_historical_values[col]\n",
    "for lag in tlags_2:\n",
    "    col = column + '_splag_2_' + str(lag)\n",
    "    df_pgm_historical_values[col] = df_pgm_historical_values[col]\n",
    "\n",
    "\n",
    "\n",
    "df_pgm_historical_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pgm_historical_values_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating predictions for test partition\n",
    "number_of_lags_inner = 24\n",
    "number_of_lags_1 = 12\n",
    "number_of_lags_2 = 6\n",
    "maxstep = 14\n",
    "df_list_bystep = []\n",
    "for step in range(3,maxstep+1):\n",
    "    lags=range(step,number_of_lags_inner+step)\n",
    "    draw = 1\n",
    "#    print('step:',step,'lag:',lag, 'draw:',draw)\n",
    "    df_list = []\n",
    "    for lag in lags:\n",
    "        for coltype in [('ged_sb_best_sum_nokgi_',number_of_lags_inner),('ged_sb_best_sum_nokgi_splag_1_',number_of_lags_1),('ged_sb_best_sum_nokgi_splag_2_',number_of_lags_2)]:\n",
    "            number_of_repetitions = coltype[1]+step-lag\n",
    "            for repetition in range(1,number_of_repetitions+1):\n",
    "                if lag <= coltype[1] + step:\n",
    "                    lagged_col = coltype[0] + str(lag)\n",
    "#                    print('draw:',draw, 'step:', step, 'lag:',lag,'repetition:', repetition, 'colname:', lagged_col)\n",
    "                    df = pd.DataFrame(df_pgm_historical_values[lagged_col].copy())\n",
    "                    df.reset_index(inplace=True)\n",
    "                    df['prediction'] = df[lagged_col].astype('int32')\n",
    "                    df.drop(columns=[lagged_col], inplace=True)\n",
    "                    df['step'] = step\n",
    "                    df['draw'] = draw\n",
    "                    df.set_index(['month_id', 'priogrid_gid', 'step','draw'], inplace=True)\n",
    "                    df_list.append(df)\n",
    "                    draw = draw + 1\n",
    "    print('Concatenating', draw-1, 'repetitions for step', step)\n",
    "    df_pgm_predictions_lag = pd.concat(df_list)\n",
    "    df_list_bystep.append(df_pgm_predictions_lag)\n",
    "    \n",
    "#df_pgm_predictions_historical_values = pd.concat(df_list_bystep) \n",
    "#df_pgm_predictions_historical_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff56a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_bystep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sc prediction files for historical values model -- step by step\n",
    "step = 3\n",
    "df_sc_bystep = [[],[],[],[]]\n",
    "for step_df in df_list_bystep:\n",
    "    print('step:', step)\n",
    "    pgm_hv_step = from_ss48_to_sc12(step_df,'pgm2',445,4)\n",
    "    per = 0\n",
    "    for period in pgm_hv_step:\n",
    "        df_sc_bystep[per].append(pgm_hv_step[per])\n",
    "        per += 1\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ade91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Aggregating')\n",
    "pgm_historical_values_predictions = [[],[],[],[]]\n",
    "for period in range(0,4):\n",
    "    pgm_historical_values_predictions[period] = pd.concat(df_sc_bystep[period]) \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the annual sc files and the aggregated versions of them\n",
    "filepath = Mydropbox + 'Prediction_competition_2023/'\n",
    "year = 2018\n",
    "for df in pgm_historical_values_predictions:\n",
    "    filename = filepath + 'bm_pgm_historical_values_' + str(year) + '.parquet'\n",
    "    print(filename)\n",
    "    df.to_parquet(filename)\n",
    "    df_aggregated = aggregate_and_categorize(df,'pgm')\n",
    "    filename = filepath + 'bm_pgm_historical_values_agg' + str(year) + '.parquet'\n",
    "    df_aggregated.to_parquet(filename)\n",
    "    year = year + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_historical_values_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ec044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating step-level dataframes\n",
    "single_file = False\n",
    "\n",
    "if single_file:\n",
    "    df_pgm_predictions_historical_values = df_list_bystep[0]\n",
    "    list_item = 1\n",
    "    for step in range(3+1,maxstep+1):\n",
    "        print('adding data for step', step)\n",
    "        df_pgm_predictions_historical_values = pd.concat([df_pgm_predictions_historical_values,df_list_bystep[list_item]])\n",
    "        list_item += 1\n",
    "\n",
    "    df_pgm_predictions_historical_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccedd48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5443cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with actuals\n",
    "df_actuals_pgm = pd.DataFrame(df_pgm_historical_values_0['ged_sb'])\n",
    "print(df_actuals_pgm.head())\n",
    "print(df_actuals_pgm.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "filename = Mydropbox + 'Prediction_competition_2023/' + 'pgm_actuals.parquet'\n",
    "df_actuals_pgm.to_parquet(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a3194",
   "metadata": {},
   "source": [
    "# Probably obsolete from here.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate across draws/samples to extract means, standard deviations, and category probabilities\n",
    "# export to parquet step by step\n",
    "step = 3\n",
    "for df in df_list_bystep:\n",
    "    print(step)\n",
    "    filename = Mydropbox + 'Prediction_competition_2023/' + 'pgm_benchmark_historical_values_step_' + str(step) + '.parquet'\n",
    "    df.to_parquet(filename)\n",
    "    filename = Mydropbox + 'Prediction_competition_2023/' + 'pgm_benchmark_historical_values_step_' + str(step) + '_aggregated.parquet'\n",
    "    df_aggregated = aggregate_and_categorize(df,'pgm')\n",
    "    df_aggregated.to_parquet(filename)\n",
    "\n",
    "    print(df_aggregated.describe())\n",
    "    print(df_aggregated.mean())\n",
    "        \n",
    "    step = step+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d69f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Mydropbox + 'Prediction_competition_2023/' + 'pgm_benchmark_historical_values.parquet'\n",
    "df_from_file = pd.read_parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Mydropbox + 'Prediction_competition_2023/' + 'pgm_benchmark_historical_values_step_3.parquet'\n",
    "df_from_file = pd.read_parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db74afc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
