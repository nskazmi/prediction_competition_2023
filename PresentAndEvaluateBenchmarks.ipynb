{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0573b9c2",
   "metadata": {},
   "source": [
    "# Presenting and evaluating benchmark models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4857d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import os\n",
    "\n",
    "# Evaluation scripts\n",
    "from CompetitionEvaluation import load_data, structure_data, calculate_metrics\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edd77af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropbox path set to /Users/root/Dropbox (ViEWS)/ViEWS/\n",
      "Overleaf path set to /Users/root/Dropbox (ViEWS)/Apps/Overleaf/Prediction competition 2023/Tables/\n"
     ]
    }
   ],
   "source": [
    "# Where to find files\n",
    "username = os.getlogin()\n",
    "Mydropbox = f'/Users/{username}/Dropbox (ViEWS)/ViEWS/'\n",
    "overleafpath = f'/Users/{username}/Dropbox (ViEWS)/Apps/Overleaf/Prediction competition 2023/Tables/'\n",
    "overleafpath_figures = f'/Users/{username}/Dropbox (ViEWS)/Apps/Overleaf/Prediction competition 2023/Figures/'\n",
    "print('Dropbox path set to',Mydropbox)\n",
    "print('Overleaf path set to',overleafpath)\n",
    "\n",
    "filepath = Mydropbox + 'Prediction_competition_2023/' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2bf775",
   "metadata": {},
   "source": [
    "## Reading in actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c1be9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/root/Dropbox (ViEWS)/ViEWS/Prediction_competition_2023/cm_actuals_allyears.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/noorainkazmi/Documents/GitHub/prediction_competition_2023/PresentAndEvaluateBenchmarks.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/noorainkazmi/Documents/GitHub/prediction_competition_2023/PresentAndEvaluateBenchmarks.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_cm_actuals \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(filepath \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mcm_actuals_allyears.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/noorainkazmi/Documents/GitHub/prediction_competition_2023/PresentAndEvaluateBenchmarks.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_pgm_actuals \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(filepath \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpgm_actuals_allyears.parquet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/noorainkazmi/Documents/GitHub/prediction_competition_2023/PresentAndEvaluateBenchmarks.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_cm_actuals\u001b[39m.\u001b[39mtail(), df_pgm_actuals\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/miniforge3/envs/viewser/lib/python3.9/site-packages/pandas/io/parquet.py:503\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[39mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39mDataFrame\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    501\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    504\u001b[0m     path,\n\u001b[1;32m    505\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m    506\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    507\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    508\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    509\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/viewser/lib/python3.9/site-packages/pandas/io/parquet.py:244\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    242\u001b[0m     to_pandas_kwargs[\u001b[39m\"\u001b[39m\u001b[39msplit_blocks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    245\u001b[0m     path,\n\u001b[1;32m    246\u001b[0m     kwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mfilesystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    248\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    249\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mread_table(\n\u001b[1;32m    252\u001b[0m         path_or_handle, columns\u001b[39m=\u001b[39mcolumns, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    253\u001b[0m     )\u001b[39m.\u001b[39mto_pandas(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mto_pandas_kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/viewser/lib/python3.9/site-packages/pandas/io/parquet.py:102\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m     92\u001b[0m handles \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     \u001b[39mnot\u001b[39;00m fs\n\u001b[1;32m     95\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     handles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m    103\u001b[0m         path_or_handle, mode, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    105\u001b[0m     fs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     path_or_handle \u001b[39m=\u001b[39m handles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniforge3/envs/viewser/lib/python3.9/site-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/root/Dropbox (ViEWS)/ViEWS/Prediction_competition_2023/cm_actuals_allyears.parquet'"
     ]
    }
   ],
   "source": [
    "df_cm_actuals = pd.read_parquet(filepath + 'cm_actuals_allyears.parquet')\n",
    "df_pgm_actuals = pd.read_parquet(filepath + 'pgm_actuals_allyears.parquet')\n",
    "df_cm_actuals.tail(), df_pgm_actuals.head()\n",
    "# Recast to int32\n",
    "df_cm_actuals['ged_sb'] = df_cm_actuals['ged_sb'].astype('int32')\n",
    "df_pgm_actuals['ged_sb'] = df_pgm_actuals['ged_sb'].astype('int32')\n",
    "df_pgm_actuals.index.set_names('priogrid_id', level=1,inplace=True)\n",
    "# Have to rename column name....:\n",
    "df_cm_actuals.rename(columns={\"ged_sb\": \"outcome\"}, errors=\"raise\", inplace=True)\n",
    "df_pgm_actuals.rename(columns={\"ged_sb\": \"outcome\"}, errors=\"raise\", inplace=True)\n",
    "# Summarize:\n",
    "print(df_cm_actuals.dtypes, df_pgm_actuals.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm_actuals.describe(percentiles=[.25,.50,.75,.90,.95,.99,.992,.995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88954d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from viewser import Queryset, Column\n",
    "# read in country list with country names for presentation purposes\n",
    "qs = (Queryset(\"country_list\", \"country_month\")\n",
    "\n",
    "   .with_column(Column(\"id\", from_table=\"country\", from_column=\"id\"))\n",
    "   .with_column(Column(\"name\", from_table=\"country\", from_column=\"name\"))\n",
    "              \n",
    "   )\n",
    "countrylist = qs.publish().fetch().loc[504]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8484fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "countrylist.loc[69]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d5877",
   "metadata": {},
   "source": [
    "## Reading in benchmark prediction models: \n",
    "\n",
    "Two models per level:\n",
    "\n",
    "1. cm model, based on ensemble\n",
    "2. cm model, based on historical values \n",
    "3. pgm model, based on ensemble\n",
    "4. pgm model, based on historical values\n",
    "\n",
    "Ëach of these have predictions for each of four years; 2019, 2020, 2021, and 2022. The four years are collected in lists of dictionaries including dataframes and some metadata, one for each of the models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cm_actuals.query('country_id == 57'))\n",
    "df_cm_actuals.loc[445:468]\n",
    "df_cm_actuals.head()\n",
    "df_cm_actuals.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd09a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_cm_ensemble_poisson = []\n",
    "bm_cm_ensemble_identical = []\n",
    "bm_cm_constituent_poisson = []\n",
    "bm_cm_actuals_bootstrap = []\n",
    "bm_cm_last_historical_poisson= []\n",
    "\n",
    "model_names = ['ensemble_poisson','ensemble_identical','constituent_poisson','bootstrap','last_historical_poisson']\n",
    "\n",
    "bm_pgm_ensemble_poisson = []\n",
    "bm_pgm_ensemble_identical = []\n",
    "bm_pgm_historical_poisson = []\n",
    "model_names = ['ensemble_poisson','ensemble_identical','historical_poisson']\n",
    "\n",
    "include_historical_values = True\n",
    "include_constituent = False\n",
    "include_pgm = True\n",
    "\n",
    "def positive_integers(df, colname):\n",
    "    df[colname] = np.round(df[colname]).astype('int32')\n",
    "    df[colname][df[colname] < 0] = 0\n",
    "    return(df)\n",
    "\n",
    "colname = 'prediction'\n",
    "colname = 'outcome'\n",
    "for year in [2018, 2019, 2020, 2021]:\n",
    "    print(year)\n",
    "    first_month = (year - 1980)*12 + 1\n",
    "    cm_e = {\n",
    "        'year': year,\n",
    "        'level': 'cm',\n",
    "        'first_month': first_month,\n",
    "        'name': 'cm_ensemble_poisson',\n",
    "        'df_full': positive_integers(pd.read_parquet(filepath + 'bm_cm_ensemble_poisson_expanded_' + str(year) + '.parquet'),colname),\n",
    "        'actuals': df_cm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "    }\n",
    "    bm_cm_ensemble_poisson.append(cm_e)\n",
    "    \n",
    "    cm_e = {\n",
    "        'year': year,\n",
    "        'level': 'cm',\n",
    "        'first_month': first_month,\n",
    "        'name': 'cm_ensemble_identical',\n",
    "        'df_full': positive_integers(pd.read_parquet(filepath + 'bm_cm_ensemble_identical_expanded_' + str(year) + '.parquet'),colname),\n",
    "        'actuals': df_cm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "    }\n",
    "    bm_cm_ensemble_identical.append(cm_e)\n",
    "    \n",
    "    cm_e = {\n",
    "        'year': year,\n",
    "        'level': 'cm',\n",
    "        'first_month': first_month,\n",
    "        'name': 'cm_constituent_poisson',\n",
    "        'df_full': positive_integers(pd.read_parquet(filepath + 'bm_cm_constituent_poisson_expanded_' + str(year) + '.parquet'),colname),\n",
    "        'actuals': df_cm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "    }\n",
    "    bm_cm_constituent_poisson.append(cm_e)\n",
    "    \n",
    "    cm_e = {\n",
    "        'year': year,\n",
    "        'level': 'cm',\n",
    "        'first_month': first_month,\n",
    "        'name': 'cm_actuals_bootstrap',\n",
    "        'df_full': positive_integers(pd.read_parquet(filepath + 'bm_cm_bootstrap_expanded_' + str(year) + '.parquet'),colname),\n",
    "        'actuals': df_cm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "    }\n",
    "    bm_cm_actuals_bootstrap.append(cm_e)\n",
    "    \n",
    "    if include_historical_values:\n",
    "        cm_hv = {\n",
    "            'year': year,\n",
    "        'level': 'cm',\n",
    "            'first_month': first_month,\n",
    "            'name': 'cm_historical_poisson',\n",
    "            'df_full': positive_integers(pd.read_parquet(filepath + 'bm_cm_last_historical_poisson_expanded_' + str(year) + '.parquet'),colname),\n",
    "            'actuals': df_cm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "        }\n",
    "        bm_cm_last_historical_poisson.append(cm_hv)\n",
    "        \n",
    "    if include_pgm:\n",
    "        pgm_ep = {\n",
    "            'year': year,\n",
    "            'level': 'pgm',\n",
    "            'first_month': first_month,\n",
    "            'name': 'pgm_ensemble_poisson',\n",
    "            'df_full': positive_integers(pd.read_parquet(filepath + 'bm_pgm_ensemble_poisson_expanded_' + str(year) + '.parquet'),colname),\n",
    "            'actuals': df_pgm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "        }\n",
    "        pgm_ep['df_full'].index.set_names('priogrid_id', level=1, inplace=True)\n",
    "        bm_pgm_ensemble_poisson.append(pgm_ep)\n",
    "        \n",
    "        pgm_ei = {\n",
    "            'year': year,\n",
    "            'level': 'pgm',\n",
    "            'first_month': first_month,\n",
    "            'name': 'pgm_ensemble_identical',\n",
    "            'df_full': positive_integers(pd.read_parquet(filepath + 'bm_pgm_ensemble_identical_expanded_' + str(year) + '.parquet'),colname),\n",
    "            'actuals': df_pgm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "        }\n",
    "        pgm_ei['df_full'].index.set_names('priogrid_id', level=1, inplace=True)\n",
    "        bm_pgm_ensemble_identical.append(pgm_ei)\n",
    "        \n",
    "        pgm_hv = {\n",
    "            'year': year,\n",
    "            'level': 'pgm',\n",
    "            'first_month': first_month,\n",
    "            'name': 'pgm_historical_poisson',\n",
    "            'df_full': positive_integers(pd.read_parquet(filepath + 'bm_pgm_historical_values_poisson_expanded_' + str(year) + '.parquet'),colname),\n",
    "            'actuals': df_pgm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "        }\n",
    "        pgm_hv['df_full'].index.set_names('priogrid_id', level=1, inplace=True)\n",
    "        bm_pgm_historical_poisson.append(pgm_hv)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ddd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_cm_ensemble_poisson[3]['df_full'].loc[494].loc[67].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9cb1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructuring, evaluating:\n",
    "\n",
    "# Evaluation parameters:\n",
    "ign_bins = [0, 0.5, 2.5, 5.5, 10.5, 25.5, 50.5, 100.5, 250.5, 500.5, 1000.5]\n",
    "#ign_bins = [0, 0.5, 1000]\n",
    "#bm_cm_constituent_poisson,\n",
    "bm_list = [bm_pgm_ensemble_poisson, bm_pgm_ensemble_identical, bm_pgm_historical_poisson, bm_cm_ensemble_poisson,bm_cm_ensemble_identical,bm_cm_last_historical_poisson,bm_cm_actuals_bootstrap]\n",
    "\n",
    "for model_list in bm_list:\n",
    "    for item in model_list:\n",
    "        print(item['name'], item['year'])\n",
    "        if item['level']=='cm':\n",
    "            spatial_unit = 'country_id'\n",
    "        elif  item['level']=='pgm':\n",
    "            spatial_unit = 'priogrid_id'\n",
    "#        print(item['df_full'].query('draw == 0').describe())\n",
    "        item['observed'], item['predictions'] = structure_data(item['actuals'], item['df_full']) # structure data as xarrays that the xskillscore.crps_ensemble wants\n",
    "        item['crps'] = calculate_metrics(item['observed'], item['predictions'], metric = 'crps', aggregate_over=['month_id', spatial_unit]) # calculates crps.\n",
    "        item['crps_by_month'] = calculate_metrics(item['observed'], item['predictions'], metric = 'crps', aggregate_over=spatial_unit) # calculates crps.\n",
    "        item['crps_by_country'] = calculate_metrics(item['observed'], item['predictions'], metric = 'crps', aggregate_over='month_id') # calculates crps.\n",
    "        item['ign'] = calculate_metrics(item['observed'], item['predictions'], metric = \"ign\", bins = ign_bins, aggregate_over=['month_id', spatial_unit])\n",
    "        item['ign_by_month'] = calculate_metrics(item['observed'], item['predictions'], metric = \"ign\", bins = ign_bins, aggregate_over=spatial_unit)\n",
    "        item['ign_by_country'] = calculate_metrics(item['observed'], item['predictions'], metric = \"ign\", bins = ign_bins, aggregate_over='month_id')\n",
    "        item['mis'] = calculate_metrics(item['observed'], item['predictions'], metric = \"mis\", aggregate_over=['month_id', spatial_unit])\n",
    "        item['mis_by_month'] = calculate_metrics(item['observed'], item['predictions'], metric = \"mis\", aggregate_over=spatial_unit)\n",
    "        item['mis_by_country'] = calculate_metrics(item['observed'], item['predictions'], metric = \"mis\", aggregate_over='month_id')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85543b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "countrylist.tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_plot = [\n",
    "    ('prediction',    '1%'),\n",
    "    ('prediction',    '5%'),\n",
    "    ('prediction',   '10%'),\n",
    "    ('prediction',   '20%'),\n",
    "    ('prediction',   '50%'),\n",
    "    ('prediction',   '80%'),\n",
    "    ('prediction',   '90%'),\n",
    "    ('prediction',   '95%'),\n",
    "    ('prediction',   '99%'),\n",
    "]\n",
    "ctp1 = [\n",
    "    ('prediction',  'mean'),]\n",
    "ctp2 = [(   'actuals',  'mean')] # Separate set for separate color/pattern\n",
    "\n",
    "\n",
    "for model_list in bm_list:\n",
    "    for item in model_list:\n",
    "        if item['level'] == 'cm':\n",
    "            df_actuals = item['actuals'].reorder_levels(['country_id','month_id'])\n",
    "            df_actuals.rename(columns={'outcome':'actuals'},inplace=True)\n",
    "            df_full = item['df_full'].reorder_levels(['country_id','month_id','draw'])\n",
    "            df_full.rename(columns={'outcome':'prediction'},inplace=True)\n",
    "            df_merged = pd.merge(df_actuals, df_full,left_index=True,right_index=True)\n",
    "            for c in [57,67,69,131,214,220,246]:\n",
    "                countryname = countrylist.loc[c]['name']\n",
    "                title = 'Model ' + item['name'] + ', ' + countryname + ' ' + str(item['year'])\n",
    "                fig, axs = plt.subplots(figsize=(16, 4))\n",
    "                df_description = df_merged.loc[c].groupby('month_id').describe(percentiles=[.01,.05,.1,.2,.5,.8,.9,.95,.99])\n",
    "                df_description[columns_to_plot].plot(use_index=True,ylabel='Battle-related deaths',ax=axs,title=title,colormap='coolwarm')\n",
    "                df_description[ctp1].plot(ax=axs,color='gray',  linestyle='dashed', linewidth=2)\n",
    "                df_description[ctp2].plot(ax=axs,color='black', linewidth=3)\n",
    "                axs.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size':8})\n",
    "                fig.savefig(overleafpath_figures+'bm_predictions/'+'predictions_and_actuals_' + countryname + '_' + str(item['year']) + '_' + item['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b39a757",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_description.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64105e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "item['df_full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7195d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = pd.concat([df_actuals,df_description], ignore_index=True)\n",
    "df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actuals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7561025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actuals.describe(), df_description.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_description.head())\n",
    "x = df_description['month_id']\n",
    "y=df_description[('outcome',   '10%')]\n",
    "z=df_description[('outcome',   '90%')]\n",
    "plt.plot(x,y,z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7296cdef",
   "metadata": {},
   "source": [
    "# Assembling evaluation tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table A for each model with:\n",
    "# one row for each year plus one for mean over years\n",
    "# one col for crps \n",
    "# one col for ign \n",
    "# one col for mis\n",
    "# Create table B with the same, but metrics per month\n",
    "\n",
    "# Table A:\n",
    "for model_list in bm_list:\n",
    "    year_list = []\n",
    "    for item in model_list:\n",
    "        metrics = pd.concat([item['crps'],item['ign'],item['mis']],axis=1)\n",
    "        metrics['year'] = item['year']\n",
    "        year_list.append(metrics)\n",
    "    table_annual = pd.concat(year_list,axis=0)   \n",
    "    table_annual.set_index(str('year'),inplace=True)\n",
    "    table_annual.loc['Mean'] = table_annual.mean()\n",
    "    table_filename = overleafpath + 'bm_evaluation/' + item['name'] + '_aggregated' + '.tex'\n",
    "    print(item['name'],table_filename)\n",
    "    with open(table_filename, 'w') as tf:\n",
    "        tf.write(table_annual.to_latex(float_format=\"{:.2f}\".format))\n",
    "\n",
    "# Table B:\n",
    "for model_list in bm_list:\n",
    "    year_list = []\n",
    "    for item in model_list:\n",
    "        metrics = pd.concat([item['crps_by_month'],item['ign_by_month'],item['mis_by_month']],axis=1)\n",
    "        metrics['month'] = metrics.index - (item['year']-1980)*12\n",
    "        metrics['month'] = metrics['month'].astype(int)\n",
    "        metrics['year'] = item['year']\n",
    "        year_list.append(metrics)\n",
    "    table_monthly = pd.concat(year_list,axis=0)   \n",
    "    table_monthly.set_index(['year','month'],inplace=True)\n",
    "    table_monthly_aggregated = table_monthly.groupby('month').agg('mean')\n",
    "    #table.loc['Mean'] = table.mean()\n",
    "    table_filename = overleafpath + 'bm_evaluation/' + item['name'] + '_monthly' + '.tex'\n",
    "    with open(table_filename, 'w') as tf:\n",
    "        tf.write(table_monthly_aggregated.to_latex(float_format=\"{:.2f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba48cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['month'] = metrics.index - (2021-1980)*12\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2aebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether balanced panel:\n",
    "\n",
    "if include_historical_values:\n",
    "    print(bm_cm_historical_values[0]['df_full'].head())\n",
    "    print('Number of missing?',bm_cm_historical_values[0]['df_full'].isnull().sum())\n",
    "    print(bm_cm_historical_values[0]['df_full'].describe())\n",
    "    for m in range(445,457):\n",
    "        df = bm_cm_historical_values[0]['df_full'].loc[m]\n",
    "        print(m,len(df))\n",
    "        df2 =df.groupby([\"country_id\"]).agg({'prediction': [\"count\"]})\n",
    "        print(df2.describe())\n",
    "    print(990*2292)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c5fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if include_historical_values:\n",
    "    print(bm_cm_historical_values[0]['df_full'].describe())\n",
    "    print(bm_cm_historical_values[0]['df_full'].query('draw == 0').describe())\n",
    "    print(bm_cm_historical_values[0]['df_full'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_pgm_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(item['df_full']))\n",
    "print(len(item['df_full'])/12)\n",
    "print(len(item['df_full'])/(12*990))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17466c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IgnoranceScore import ensemble_ignorance_score, _ensemble_ignorance_score\n",
    "import numpy as np\n",
    "observations = [0, 1, 50, 500]\n",
    "forecasts = np.array([[0, 0, 0, 0, 0],\n",
    "                      [1, 1, 1, 2, 55],\n",
    "                      [500, 49, 52, 52, 500],\n",
    "                      [49, 49, 49, 49, 500]])\n",
    "bins = [0, 0.5, 10.5, 50.5, 100.5, 1000.5]\n",
    "res = ensemble_ignorance_score(observations, forecasts, prob_type=3, ign_max=None, round_values=False, axis=-1, bins = bins, low_bin=0, high_bin=1000)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CompetitionEvaluation import load_data, structure_data, calculate_metrics\n",
    " \n",
    "observed, predictions = load_data(forecasts_path=filepath + \"cm_benchmark_ensemble_550.parquet\",\n",
    "                                    observed_path=filepath + \"cm_actuals.parquet\")\n",
    "predictions[\"prediction\"] = predictions[\"prediction\"].replace(-1, 0)\n",
    "observed, predictions = structure_data(observed, predictions)\n",
    "metrics = calculate_metrics(observed, predictions, metric = \"ign\", round_values = True)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(calculate_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7781c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\n",
    "#observed, predictions = load_data(args.o, args.p) # read parquet files to pandas\n",
    "observed, predictions = structure_data(df_pgm_actuals, df_bm_pgm_historical_values) # structure data as xarrays that the xskillscore.crps_ensemble wants\n",
    "metrics = calculate_metrics(observed, predictions) # calculates crps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm_cm_ensemble = pd.read_parquet(filepath + 'cm_benchmark_ensemble_550.parquet')\n",
    "df_bm_cm_ensemble.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd111b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm_cm_ensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805136d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49577f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm_pgm_historical_values = pd.read_parquet(filepath + 'pgm_benchmark_historical_values_step_3.parquet')\n",
    "df_bm_pgm_historical_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd7776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#observed, predictions = load_data(args.o, args.p) # read parquet files to pandas\n",
    "observed, predictions = structure_data(df_cm_actuals, df_bm_cm_ensemble) # structure data as xarrays that the xskillscore.crps_ensemble wants\n",
    "metrics = calculate_metrics(observed, predictions) # calculates crps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in for all 12 steps\n",
    "from datetime import datetime\n",
    "print(\"Cell started to run:\", datetime.now())\n",
    "\n",
    "df_pgm_hv = []\n",
    "for step in range(3,14+1):\n",
    "    df = pd.read_parquet(filepath + 'pgm_benchmark_historical_values_step_' + str(step) + '.parquet')\n",
    "    print(step, df.describe())\n",
    "    df_pgm_hv.append(df)\n",
    "    \n",
    "print(\"Cell run ended:\", datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cell started to run:\", datetime.now())\n",
    "i = 3\n",
    "for df in df_pgm_hv:\n",
    "    print('step',i,datetime.now())\n",
    "    observed, predictions = structure_data(df_pgm_actuals, df) # structure data as xarrays that the xskillscore.crps_ensemble wants\n",
    "    metrics = calculate_metrics(observed, predictions) # calculates crps.\n",
    "    print(metrics)\n",
    "    i=i+1\n",
    "print(\"Cell run ended:\", datetime.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b41a7",
   "metadata": {},
   "source": [
    "# Read in the sc-type prediction files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm_pgm_ensemble2022 = pd.read_parquet(filepath + 'bm_pgm_ensemble_2022.parquet')\n",
    "df_pgm_actuals_2022 = df_pgm_actuals.loc[505:516]\n",
    "df_bm_pgm_ensemble2022.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "observed, predictions = structure_data(df_pgm_actuals_2022, df_bm_pgm_ensemble2022) # structure data as xarrays that the xskillscore.crps_ensemble wants\n",
    "metrics = calculate_metrics(observed, predictions) # calculates crps.\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65dc5cc",
   "metadata": {},
   "source": [
    "# Creating samples based on point predictions\n",
    "\n",
    "Assuming Poisson distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_ensemble_aggregated = pd.read_parquet(filepath + 'cm_benchmark_ensemble_550_aggregated.parquet')\n",
    "\n",
    "print(cm_ensemble_aggregated.describe())\n",
    "print(cm_ensemble_aggregated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip down to a year of sc predictions:\n",
    "df_cm_ensemble = []\n",
    "for step in range(3,14+1):\n",
    "    df = cm_ensemble_aggregated['mean_log_prediction'].loc[442+step]\n",
    "    df = pd.DataFrame(df[df.index.get_level_values('step').isin([step])])\n",
    "    df['prediction'] = np.expm1(df['mean_log_prediction'])\n",
    "    df_cm_ensemble.append(df)\n",
    "\n",
    "df_cm_ensemble_stripped = pd.concat(df_cm_ensemble)\n",
    "print(df_cm_ensemble_stripped.describe())\n",
    "print(df_cm_ensemble_stripped.tail(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
